{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1ea812a-c1a4-4a74-80cd-7043891e3b7d",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7a3f6f-7725-4d78-9f44-1184356611ca",
   "metadata": {},
   "source": [
    "A decision tree is a popular machine-learning algorithm used for both classification and regression tasks. It models decisions and their possible consequences as a tree-like structure, where each internal node represents a \"test\" on an attribute, each branch represents the outcome of the test, and each leaf node represents a class label (in classification) or a continuous output (in regression).\n",
    "\n",
    "### Key Components of a Decision Tree:\n",
    "1. Nodes: There are two types of nodes in a decision tree i.e. Decision Nodes, Leaf Nodes\n",
    "2. Branches: These are the links between nodes, representing the decision rules or outcomes of the tests performed at the decision nodes.\n",
    "\n",
    "### How a Decision Tree Works:\n",
    "- Splitting: Starting from the root of the tree, the data is split into subsets based on a feature that results in the highest \"purity\" (homogeneity) of the subsets. This is done using a criterion such as the gini index or the entropy in classification, and variance reduction in regression.\n",
    "- Recursive Partitioning: This process is repeated recursively for each derived subset in a recursive manner. The recursion is completed when a stopping condition is met. Typical stopping conditions are a maximum tree depth, minimum number of samples in a node, or no further improvement in purity.\n",
    "- Pruning: Sometimes, a fully grown tree is \"pruned\" by removing parts of the tree that provide little power in predicting target variables, to reduce overfitting and simplify the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7174fe9b-bebc-4d04-a9fd-2e854f5d58a1",
   "metadata": {},
   "source": [
    "**Creating a decision tree classifier from scratch** involves a few core components like calculating the best split based on a metric (like Gini impurity or entropy), recursively splitting the data, and then building the tree structure.\n",
    "\n",
    "The Node class serves as the building block of the decision tree. Each node represents a decision point and can either be a decision node or a leaf node. \n",
    "\n",
    "To determine this each node is accounted with a feature index i.e., the index of the feature used for splitting the data at this node, threshold which is the value at which the split is made if the node is a decision node, left and right attributes that points to the left and right child of the node respectively and a value which is set only for leaf nodes and contains the class label that is most frequent in the data that reaches this leaf.\n",
    "\n",
    "Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset. The formula for Gini impurity is:\n",
    "\n",
    "                   J\n",
    "    Gini(y) = 1 − ∑   Pi^2\n",
    "                   i=1\n",
    "\n",
    "where pi, is the proportion of class i elements in the set. \n",
    "\n",
    "Lower Gini impurity indicates a more \"pure\" node with predominant elements from a single class, which is the desired outcome in decision trees.\n",
    "\n",
    "To find the best feature and threshold to split the data, we iterate through all features and all possible thresholds (by sorting unique values of a feature) and for each threshold, the data is divided into left and right subsets. Then the gini impurity is calculated for each subset, and a weighted average of these impurities is computed based on the size of the subsets. Finally, the split that results in the highest reduction in impurity (lowest weighted Gini impurity) is chosen as the best split.\n",
    "\n",
    "The decision tree is created recursively by first checking whether the recursion should stop, which happens if all data at the node belong to one class or the maximum depth is reached, if the stopping condition is not met, it finds the best split to determine the best way to split the data. It then splits the data into left and right subsets and recursively calls itself to process these subsets. This recursion ends when it reaches a leaf node, where it returns a Node with the most frequent class in the subset.\n",
    "\n",
    "For predictions, the tree is transversed for any input data starting from the root node, and based on the feature's index value and threshold it decides to move towards the left or right child. This process is repeated until a leaf node is reached, whose value attributes give the predicted class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c36ee1a-7b53-4a6b-8e8b-f7220547e3bc",
   "metadata": {},
   "source": [
    "Below, I have provided an implementation using gini impurity as the metric to choose the splits. \n",
    "\n",
    "This example is a simplified version that assumes all features are numerical and the target is binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90cf65c3-0a61-4827-a4f1-1d3b8eae857d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# !pip install ucimlrepo\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "855f825d-396e-4f42-bc24-f6e9ff2fa90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node Class represents a single decision node or a leaf in the decision tree.\n",
    "class Node:\n",
    "    # A Node in the Decision Tree.\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, *, value=None):\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9e06491-5299-436f-b0f8-c624a1be5485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the gini impurity of a node.\n",
    "def gini_impurity(y):\n",
    "    class_probs = np.bincount(y) / len(y)\n",
    "    return 1 - np.sum(class_probs**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "551865b7-4a9d-4b88-bad8-3db4ef5476c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to identify the optimal place to split the feature set to minimize impurity.\n",
    "def best_split(X, y):\n",
    "    m, n = X.shape\n",
    "    best_gini = 1.0  # Starting with a worst-case scenario of maximum impurity\n",
    "    best_idx, best_thr = None, None\n",
    "\n",
    "    for index in range(n):  # Iterating over each feature\n",
    "        thresholds, classes = zip(*sorted(zip(X[:, index], y)))  # Sort data along the feature\n",
    "        \n",
    "        # Ensuring that bincount arrays accommodate all class labels\n",
    "        max_label = np.max(y) + 1\n",
    "        left_classes = np.zeros(max_label, dtype=int)\n",
    "        right_classes = np.bincount(classes, minlength=max_label)\n",
    "        \n",
    "        for i in range(1, m):  # Iterating over each threshold\n",
    "            c = classes[i - 1]\n",
    "            left_classes[c] += 1\n",
    "            right_classes[c] -= 1\n",
    "            \n",
    "            if i < m and thresholds[i] != thresholds[i - 1]:  # Only calculate Gini if the threshold changes\n",
    "                gini_left = gini_impurity(left_classes)\n",
    "                gini_right = gini_impurity(right_classes)\n",
    "                # Weighted Gini for left and right nodes\n",
    "                gini = (i * gini_left + (m - i) * gini_right) / m\n",
    "                \n",
    "                if gini < best_gini:\n",
    "                    best_gini = gini\n",
    "                    best_idx, best_thr = index, (thresholds[i] + thresholds[i - 1]) / 2\n",
    "\n",
    "    return best_idx, best_thr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd48b57c-8cf1-405d-9381-f3ce2080cd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build tree function recursively splits the data until the stopping criteria are met (e.g., max depth or minimal impurity gain).\n",
    "# i.e. it builds the decision tree recursively.\n",
    "def build_tree(X, y, depth=0, max_depth=10):\n",
    "    num_samples_per_class = np.bincount(y)\n",
    "    # Checking if only one class left or max depth reached\n",
    "    if num_samples_per_class.size == 1 or depth == max_depth:\n",
    "        # Returning a leaf node with class label\n",
    "        leaf_value = np.argmax(num_samples_per_class)\n",
    "        return Node(value=leaf_value)\n",
    "\n",
    "    # Finding the best split\n",
    "    feature_index, threshold = best_split(X, y)\n",
    "    if feature_index is None:\n",
    "        # Returning a leaf node if no split improves impurity\n",
    "        leaf_value = np.argmax(num_samples_per_class)\n",
    "        return Node(value=leaf_value)\n",
    "\n",
    "    # Splitting the dataset\n",
    "    left_idxs = X[:, feature_index] < threshold\n",
    "    right_idxs = X[:, feature_index] >= threshold\n",
    "    left_subtree = build_tree(X[left_idxs], y[left_idxs], depth + 1, max_depth)\n",
    "    right_subtree = build_tree(X[right_idxs], y[right_idxs], depth + 1, max_depth)\n",
    "    \n",
    "    return Node(feature_index, threshold, left_subtree, right_subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39cc2f4e-05bc-4134-acb9-d4c942212416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicts the class label for a given sample using the tree\n",
    "def predict(node, X):\n",
    "    while node.value is None:\n",
    "        try:\n",
    "            feature_value = float(X[node.feature_index])\n",
    "            threshold = float(node.threshold)\n",
    "        except ValueError:\n",
    "            print(f\"Warning: Non-numeric data encountered: {X[node.feature_index]}\")\n",
    "            return None  # Or handle as needed\n",
    "        \n",
    "        if feature_value < threshold:\n",
    "            node = node.left\n",
    "        else:\n",
    "            node = node.right\n",
    "    return node.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7512a69f-7e02-4114-8742-b386e1843406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting builds the decision tree model by calling build_tree on the training data.\n",
    "def fit(X, y):\n",
    "# Ensuring that X and y are NumPy arrays of the right type\n",
    "    y = np.array(y).flatten()  # Flattenning y to ensure it is 1D\n",
    "    y = np.array(y, dtype=int)  # Ensuring y is of integer type\n",
    "    X = np.array(X)\n",
    "    return build_tree(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "016389fb-5f04-4006-8474-5ec11319cdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting labels for a dataset while ensuring data is passed correctly to predict.\n",
    "def predict_labels(tree, X):\n",
    "    return np.array([predict(tree, xi) if not any(isinstance(x, str) for x in xi) else None for xi in X.values])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a1aa95-1029-42e6-b059-9ef095f8cccb",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1678cb73-3810-4216-9d43-669d467dcfb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6497, 12)\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset either from the online repository directly or through the local storage\n",
    "\n",
    "\n",
    "# # Fetching the data from the local storage\n",
    "# data1 = pd.read_csv('winequality-red.csv', sep=';')\n",
    "# data2 = pd.read_csv('winequality-white.csv', sep=';')\n",
    "\n",
    "# # Combining the red and white wine data\n",
    "# data = pd.concat([data1, data2])\n",
    "# print(data.shape)\n",
    "\n",
    "# # Determining the features and target varibale from \n",
    "# X = data[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphatesm', 'alcohol']]\n",
    "# y = data['quality']\n",
    "\n",
    "\n",
    "# fetching the dataset from the online UCI Machine Learning Repository \n",
    "wine_quality = fetch_ucirepo(id=186) \n",
    "\n",
    "# data (as pandas dataframes) \n",
    "X = wine_quality.data.features \n",
    "y = wine_quality.data.targets \n",
    "\n",
    "data = pd.merge(X, y, left_index=True, right_index=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc6da92d-3e75-43f2-b371-fe1f19b8bf74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixed_acidity           0\n",
      "volatile_acidity        0\n",
      "citric_acid             0\n",
      "residual_sugar          0\n",
      "chlorides               0\n",
      "free_sulfur_dioxide     0\n",
      "total_sulfur_dioxide    0\n",
      "density                 0\n",
      "pH                      0\n",
      "sulphates               0\n",
      "alcohol                 0\n",
      "quality                 0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Checking for any missing or null values in the dataset\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b62f446-3d85-4c69-a32f-98dfbabb770e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6497 entries, 0 to 6496\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   fixed_acidity         6497 non-null   float64\n",
      " 1   volatile_acidity      6497 non-null   float64\n",
      " 2   citric_acid           6497 non-null   float64\n",
      " 3   residual_sugar        6497 non-null   float64\n",
      " 4   chlorides             6497 non-null   float64\n",
      " 5   free_sulfur_dioxide   6497 non-null   float64\n",
      " 6   total_sulfur_dioxide  6497 non-null   float64\n",
      " 7   density               6497 non-null   float64\n",
      " 8   pH                    6497 non-null   float64\n",
      " 9   sulphates             6497 non-null   float64\n",
      " 10  alcohol               6497 non-null   float64\n",
      " 11  quality               6497 non-null   int64  \n",
      "dtypes: float64(11), int64(1)\n",
      "memory usage: 609.2 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Looking at the overview of the dataset's columns\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6dec9cc1-ff0a-42d1-b3ea-44d8e844553a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed_acidity</th>\n",
       "      <th>volatile_acidity</th>\n",
       "      <th>citric_acid</th>\n",
       "      <th>residual_sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free_sulfur_dioxide</th>\n",
       "      <th>total_sulfur_dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "      <td>6497.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.215307</td>\n",
       "      <td>0.339666</td>\n",
       "      <td>0.318633</td>\n",
       "      <td>5.443235</td>\n",
       "      <td>0.056034</td>\n",
       "      <td>30.525319</td>\n",
       "      <td>115.744574</td>\n",
       "      <td>0.994697</td>\n",
       "      <td>3.218501</td>\n",
       "      <td>0.531268</td>\n",
       "      <td>10.491801</td>\n",
       "      <td>5.818378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.296434</td>\n",
       "      <td>0.164636</td>\n",
       "      <td>0.145318</td>\n",
       "      <td>4.757804</td>\n",
       "      <td>0.035034</td>\n",
       "      <td>17.749400</td>\n",
       "      <td>56.521855</td>\n",
       "      <td>0.002999</td>\n",
       "      <td>0.160787</td>\n",
       "      <td>0.148806</td>\n",
       "      <td>1.192712</td>\n",
       "      <td>0.873255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.800000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.987110</td>\n",
       "      <td>2.720000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.038000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>0.992340</td>\n",
       "      <td>3.110000</td>\n",
       "      <td>0.430000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.047000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>0.994890</td>\n",
       "      <td>3.210000</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>10.300000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.700000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>8.100000</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>0.996990</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>11.300000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.900000</td>\n",
       "      <td>1.580000</td>\n",
       "      <td>1.660000</td>\n",
       "      <td>65.800000</td>\n",
       "      <td>0.611000</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>440.000000</td>\n",
       "      <td>1.038980</td>\n",
       "      <td>4.010000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>14.900000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fixed_acidity  volatile_acidity  citric_acid  residual_sugar  \\\n",
       "count    6497.000000       6497.000000  6497.000000     6497.000000   \n",
       "mean        7.215307          0.339666     0.318633        5.443235   \n",
       "std         1.296434          0.164636     0.145318        4.757804   \n",
       "min         3.800000          0.080000     0.000000        0.600000   \n",
       "25%         6.400000          0.230000     0.250000        1.800000   \n",
       "50%         7.000000          0.290000     0.310000        3.000000   \n",
       "75%         7.700000          0.400000     0.390000        8.100000   \n",
       "max        15.900000          1.580000     1.660000       65.800000   \n",
       "\n",
       "         chlorides  free_sulfur_dioxide  total_sulfur_dioxide      density  \\\n",
       "count  6497.000000          6497.000000           6497.000000  6497.000000   \n",
       "mean      0.056034            30.525319            115.744574     0.994697   \n",
       "std       0.035034            17.749400             56.521855     0.002999   \n",
       "min       0.009000             1.000000              6.000000     0.987110   \n",
       "25%       0.038000            17.000000             77.000000     0.992340   \n",
       "50%       0.047000            29.000000            118.000000     0.994890   \n",
       "75%       0.065000            41.000000            156.000000     0.996990   \n",
       "max       0.611000           289.000000            440.000000     1.038980   \n",
       "\n",
       "                pH    sulphates      alcohol      quality  \n",
       "count  6497.000000  6497.000000  6497.000000  6497.000000  \n",
       "mean      3.218501     0.531268    10.491801     5.818378  \n",
       "std       0.160787     0.148806     1.192712     0.873255  \n",
       "min       2.720000     0.220000     8.000000     3.000000  \n",
       "25%       3.110000     0.430000     9.500000     5.000000  \n",
       "50%       3.210000     0.510000    10.300000     6.000000  \n",
       "75%       3.320000     0.600000    11.300000     6.000000  \n",
       "max       4.010000     2.000000    14.900000     9.000000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the summary of the dataset\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c278eb4-36c7-4168-ae1b-7e29ea3ebb6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 5, 6, 7, 8, 9], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at the uniques values in the 'quality' feature of the dataset\n",
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "118e17ee-6127-4a2d-95f2-f4e2b59ab0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the multi-class target into a binary classification problem\n",
    "y = (y >= 7).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fd54257-13ef-492f-bdfd-978b32650958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47897fac-c007-4686-a5dd-2bc9de3ba4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the decision tree\n",
    "tree = fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e231d07c-37ba-41da-897a-b43d9adf46eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on the test set\n",
    "predictions = predict_labels(tree, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba6e470e-9770-4727-8001-8b68d3892d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the decision tree on wine quality classification: 0.81\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy of the decision tree on wine quality classification: {accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
